{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Natural Language Processing Tensorflow Week 1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPBdv48LlyqzKPM7wuLjdD3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6-5zs3KW_YoG"},"source":["#Week 1\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BdlU2VFFH8yP","executionInfo":{"status":"ok","timestamp":1634304745597,"user_tz":-60,"elapsed":1114,"user":{"displayName":"Breadboy Kid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIQJbgS8wbsEiEbFPVlrhQKwF9c9Wja8OVunfx=s64","userId":"05122309557720442969"}},"outputId":"a1dc5388-3e19-4de2-e59a-54ccaeab5441"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","# drive.mount('/content/drive/MyDrive/Coursera/Natural Language Processing Tensorflow', force_remount=True)\n","%cd /content/drive/MyDrive/Coursera/Natural Language Processing Tensorflow"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Coursera/Natural Language Processing Tensorflow\n"]}]},{"cell_type":"markdown","metadata":{"id":"_4gEY-12_jgN"},"source":["## Using APIs\n","Building up a dictionary to make a \"corpus\""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWiS4hjk_PO_","executionInfo":{"status":"ok","timestamp":1634302266539,"user_tz":-60,"elapsed":321,"user":{"displayName":"Breadboy Kid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIQJbgS8wbsEiEbFPVlrhQKwF9c9Wja8OVunfx=s64","userId":"05122309557720442969"}},"outputId":"9ffc84c8-c3c6-462f-bbaa-cecf6dce1022"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentences = [\n","             'I love my dog',\n","             'I love my cat',\n","             'You love my dog!'\n","]\n","\n","tokenizer = Tokenizer(num_words=100)\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","print(word_index)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"]}]},{"cell_type":"markdown","metadata":{"id":"Z8j1r4a8BPFE"},"source":["## Text to Sequence\n","list sequences different size so requires padding"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wAL_ubdsBOPR","executionInfo":{"status":"ok","timestamp":1634302724642,"user_tz":-60,"elapsed":303,"user":{"displayName":"Breadboy Kid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIQJbgS8wbsEiEbFPVlrhQKwF9c9Wja8OVunfx=s64","userId":"05122309557720442969"}},"outputId":"80253025-c4a8-4723-d9a5-4d718a729dfd"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentences = [\n","             'I love my dog',\n","             'I love my cat',\n","             'You love my dog!',\n","             'Do you think my dog is amazing?'\n","]\n","\n","tokenizer = Tokenizer(num_words=100)\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","print(word_index)\n","print(sequences)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n","[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQusd8WeCk3P","executionInfo":{"status":"ok","timestamp":1634302888773,"user_tz":-60,"elapsed":283,"user":{"displayName":"Breadboy Kid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIQJbgS8wbsEiEbFPVlrhQKwF9c9Wja8OVunfx=s64","userId":"05122309557720442969"}},"outputId":"491f882d-eff0-4230-caeb-e797c4b2f1e4"},"source":["test_data = [\n","             'i really love my dog',\n","             'my dog loves my manatee'\n","]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","\n","# Notice how some words are lost as not encoded into the train tokenizer\n","print(test_seq)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[[4, 2, 1, 3], [1, 3, 1]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"AJfQFrwTC-Zm"},"source":["## Looking more at the Tokenizer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1zohtmzQDA0m","executionInfo":{"status":"ok","timestamp":1634303095914,"user_tz":-60,"elapsed":300,"user":{"displayName":"Breadboy Kid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIQJbgS8wbsEiEbFPVlrhQKwF9c9Wja8OVunfx=s64","userId":"05122309557720442969"}},"outputId":"e31174ab-6666-4212-dab9-92d7a36e51f1"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentences = [\n","             'I love my dog',\n","             'I love my cat',\n","             'You love my dog!',\n","             'Do you think my dog is amazing?'\n","]\n","\n","tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","test_data = [\n","             'i really love my dog',\n","             'my dog loves my manatee'\n","]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","\n","print(word_index)\n","print(test_seq)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n","[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"bwS3ogktDwYo"},"source":["## Padding\n","Requires same size to be fed into neural network"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6BGizLED1yu","executionInfo":{"status":"ok","timestamp":1634303487311,"user_tz":-60,"elapsed":703,"user":{"displayName":"Breadboy Kid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIQJbgS8wbsEiEbFPVlrhQKwF9c9Wja8OVunfx=s64","userId":"05122309557720442969"}},"outputId":"5b6c2224-e605-4eb8-d892-dcfd8e00907d"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","sentences = [\n","             'I love my dog',\n","             'I love my cat',\n","             'You love my dog!',\n","             'Do you think my dog is amazing?'\n","]\n","\n","tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","# Max len will truncate from beginning if sentence is longer. Set truncating='post' for truncating at the end\n","# padded = pad_sequences(sequences, padding='post', maxlen=5, truncating='post')\n","padded = pad_sequences(sequences, padding='post')\n","\n","print(word_index)\n","print(sequences)\n","print(padded)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n","[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n","[[ 5  3  2  4  0  0  0]\n"," [ 5  3  2  7  0  0  0]\n"," [ 6  3  2  4  0  0  0]\n"," [ 8  6  9  2  4 10 11]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"OumS2b-HKHbd"},"source":["## Sarcasam Really?\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6o3TJthKI6u","executionInfo":{"status":"ok","timestamp":1634314027960,"user_tz":-60,"elapsed":2123,"user":{"displayName":"Breadboy Kid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIQJbgS8wbsEiEbFPVlrhQKwF9c9Wja8OVunfx=s64","userId":"05122309557720442969"}},"outputId":"4aa5a094-cf6c-44e4-d622-d2a9a91516f8"},"source":["!gdown --id 1xRU3xY5-tkiPGvlz5xBJ18_pHWSRzI4v\n","  \n","import json\n","\n","with open(\"./sarcasm.json\", 'r') as f:\n","    datastore = json.load(f)\n","\n","datastore\n","\n","sentences = []\n","labels = []\n","urls = []\n","\n","for item in datastore:\n","    sentences.append(item['headline'])\n","    labels.append(item['is_sarcastic'])\n","    urls.append(item['article_link'])"],"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1xRU3xY5-tkiPGvlz5xBJ18_pHWSRzI4v\n","To: /content/drive/MyDrive/Coursera/Natural Language Processing Tensorflow/sarcasm.json\n","\r  0% 0.00/5.64M [00:00<?, ?B/s]\r100% 5.64M/5.64M [00:00<00:00, 87.5MB/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n9Vre33KSIgz","executionInfo":{"status":"ok","timestamp":1634314033570,"user_tz":-60,"elapsed":1451,"user":{"displayName":"Breadboy Kid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIQJbgS8wbsEiEbFPVlrhQKwF9c9Wja8OVunfx=s64","userId":"05122309557720442969"}},"outputId":"fb53a302-04a6-4f00-f977-524d9f724eed"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer = Tokenizer(oov_token='<OOV>')\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences, padding='post')\n","\n","print(sentences[0])\n","print(padded[0])\n","print(padded.shape)\n","\n","# Word index are sorted by commonality"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["former versace store clerk sues over secret 'black code' for minority shoppers\n","[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0]\n","(26709, 40)\n"]}]},{"cell_type":"code","metadata":{"id":"CHFx8NHsNX9v"},"source":[""],"execution_count":null,"outputs":[]}]}